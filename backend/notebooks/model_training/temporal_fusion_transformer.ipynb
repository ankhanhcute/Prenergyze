{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "db92e36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(os.getcwd()).parent.parent.parent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a19558bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(BASE_DIR, 'backend', 'data', 'processed', 'FEATURE_ENGINEERED_DATASET.csv')\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fe1aa283",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Sort by date to ensure chronological order\n",
    "data = data.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Create time_idx - required by TimeSeriesDataSet (sequential integer index)\n",
    "data['time_idx'] = range(len(data))\n",
    "\n",
    "# Create a dummy group_id for single time series (all rows get same ID)\n",
    "data['group_id'] = 0\n",
    "\n",
    "# Create time-based features for static encoding (optional but helpful)\n",
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.month\n",
    "data['day'] = data['date'].dt.day\n",
    "data['hour'] = data['date'].dt.hour\n",
    "data['day_of_week'] = data['date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0b047a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10943 entries, 0 to 10942\n",
      "Data columns (total 69 columns):\n",
      " #   Column                                   Non-Null Count  Dtype         \n",
      "---  ------                                   --------------  -----         \n",
      " 0   date                                     10943 non-null  datetime64[ns]\n",
      " 1   load                                     10943 non-null  float64       \n",
      " 2   temperature_2m                           10943 non-null  float64       \n",
      " 3   apparent_temperature                     10943 non-null  float64       \n",
      " 4   relative_humidity_2m                     10943 non-null  float64       \n",
      " 5   vapour_pressure_deficit                  10943 non-null  float64       \n",
      " 6   pressure_msl                             10943 non-null  float64       \n",
      " 7   precipitation                            10943 non-null  float64       \n",
      " 8   cloud_cover                              10943 non-null  float64       \n",
      " 9   cloud_cover_low                          10943 non-null  float64       \n",
      " 10  cloud_cover_mid                          10943 non-null  float64       \n",
      " 11  cloud_cover_high                         10943 non-null  float64       \n",
      " 12  et0_fao_evapotranspiration               10943 non-null  float64       \n",
      " 13  sunshine_duration                        10943 non-null  float64       \n",
      " 14  wind_speed_10m                           10943 non-null  float64       \n",
      " 15  wind_gusts_10m                           10943 non-null  float64       \n",
      " 16  hour_sin                                 10943 non-null  float64       \n",
      " 17  hour_cos                                 10943 non-null  float64       \n",
      " 18  dow_sin                                  10943 non-null  float64       \n",
      " 19  dow_cos                                  10943 non-null  float64       \n",
      " 20  month_sin                                10943 non-null  float64       \n",
      " 21  month_cos                                10943 non-null  float64       \n",
      " 22  wind_dir_cos_10m                         10943 non-null  float64       \n",
      " 23  wind_dir_sin_10m                         10943 non-null  float64       \n",
      " 24  wind_dir_cos_10m_roll_mean_3h            10943 non-null  float64       \n",
      " 25  wind_dir_sin_10m_roll_mean_3h            10943 non-null  float64       \n",
      " 26  load_lag_1h                              10943 non-null  float64       \n",
      " 27  load_lag_2h                              10943 non-null  float64       \n",
      " 28  load_lag_3h                              10943 non-null  float64       \n",
      " 29  load_lag_24h                             10943 non-null  float64       \n",
      " 30  load_lag_25h                             10943 non-null  float64       \n",
      " 31  load_lag_168h                            10943 non-null  float64       \n",
      " 32  load_roll_std_3h                         10943 non-null  float64       \n",
      " 33  load_roll_std_24h                        10943 non-null  float64       \n",
      " 34  load_roll_mean_24h                       10943 non-null  float64       \n",
      " 35  load_roll_std_168h                       10943 non-null  float64       \n",
      " 36  load_roll_mean_168h                      10943 non-null  float64       \n",
      " 37  pressure_msl_lag_24h                     10943 non-null  float64       \n",
      " 38  temperature_2m_lag_24h                   10943 non-null  float64       \n",
      " 39  relative_humidity_2m_lag_1h              10943 non-null  float64       \n",
      " 40  apparent_temperature_lag_24h             10943 non-null  float64       \n",
      " 41  relative_humidity_2m_lag_24h             10943 non-null  float64       \n",
      " 42  vapour_pressure_deficit_lag_1h           10943 non-null  float64       \n",
      " 43  vapour_pressure_deficit_lag_24h          10943 non-null  float64       \n",
      " 44  pressure_msl_roll_mean_3h                10943 non-null  float64       \n",
      " 45  pressure_msl_roll_mean_24h               10943 non-null  float64       \n",
      " 46  temperature_2m_roll_mean_3h              10943 non-null  float64       \n",
      " 47  temperature_2m_roll_mean_24h             10943 non-null  float64       \n",
      " 48  apparent_temperature_roll_mean_3h        10943 non-null  float64       \n",
      " 49  relative_humidity_2m_roll_mean_3h        10943 non-null  float64       \n",
      " 50  relative_humidity_2m_roll_mean_24h       10943 non-null  float64       \n",
      " 51  vapour_pressure_deficit_roll_mean_3h     10943 non-null  float64       \n",
      " 52  vapour_pressure_deficit_roll_mean_24h    10943 non-null  float64       \n",
      " 53  cloud_cover_roll_mean_3h                 10943 non-null  float64       \n",
      " 54  wind_gusts_10m_roll_mean_3h              10943 non-null  float64       \n",
      " 55  wind_speed_10m_roll_mean_3h              10943 non-null  float64       \n",
      " 56  cloud_cover_low_roll_mean_3h             10943 non-null  float64       \n",
      " 57  cloud_cover_mid_roll_mean_3h             10943 non-null  float64       \n",
      " 58  cloud_cover_high_roll_mean_3h            10943 non-null  float64       \n",
      " 59  sunshine_duration_roll_mean_3h           10943 non-null  float64       \n",
      " 60  et0_fao_evapotranspiration_roll_mean_3h  10943 non-null  float64       \n",
      " 61  precipitation_roll_sum_3h                10943 non-null  float64       \n",
      " 62  time_idx                                 10943 non-null  int64         \n",
      " 63  group_id                                 10943 non-null  int64         \n",
      " 64  year                                     10943 non-null  int32         \n",
      " 65  month                                    10943 non-null  int32         \n",
      " 66  day                                      10943 non-null  int32         \n",
      " 67  hour                                     10943 non-null  int32         \n",
      " 68  day_of_week                              10943 non-null  int32         \n",
      "dtypes: datetime64[ns](1), float64(61), int32(5), int64(2)\n",
      "memory usage: 5.6 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b06a5c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def past_mean(series, window):\n",
    "    return series.shift(1).rolling(window, min_periods=1).mean()\n",
    "\n",
    "def past_std(series, window):\n",
    "    return series.shift(1).rolling(window, min_periods=1).std()\n",
    "    \n",
    "def past_sum(series, window):\n",
    "    return series.shift(1).rolling(window, min_periods=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "330b759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shifting all the features to avoid data leakage\n",
    "# Since we want the current row to only contain info from the past\n",
    "\n",
    "# load-based\n",
    "data['load_roll_std_3h']    = past_std(data['load'], 3)\n",
    "data['load_roll_std_24h']   = past_std(data['load'], 24)\n",
    "data['load_roll_std_168h']  = past_std(data['load'], 168)\n",
    "data['load_roll_mean_24h']  = past_mean(data['load'], 24)\n",
    "data['load_roll_mean_168h'] = past_mean(data['load'], 168)\n",
    "\n",
    "# wind direction\n",
    "data['wind_dir_cos_10m_roll_mean_3h'] = past_mean(data['wind_dir_cos_10m'], 3)\n",
    "data['wind_dir_sin_10m_roll_mean_3h'] = past_mean(data['wind_dir_sin_10m'], 3)\n",
    "\n",
    "# pressure & temperature\n",
    "data['pressure_msl_roll_mean_3h']  = past_mean(data['pressure_msl'], 3)\n",
    "data['pressure_msl_roll_mean_24h'] = past_mean(data['pressure_msl'], 24)\n",
    "data['temperature_2m_roll_mean_3h']  = past_mean(data['temperature_2m'], 3)\n",
    "data['temperature_2m_roll_mean_24h'] = past_mean(data['temperature_2m'], 24)\n",
    "\n",
    "# apparent temp & humidity\n",
    "data['apparent_temperature_roll_mean_3h']  = past_mean(data['apparent_temperature'], 3)\n",
    "data['relative_humidity_2m_roll_mean_3h']  = past_mean(data['relative_humidity_2m'], 3)\n",
    "data['relative_humidity_2m_roll_mean_24h'] = past_mean(data['relative_humidity_2m'], 24)\n",
    "\n",
    "# VPD\n",
    "data['vapour_pressure_deficit_roll_mean_3h']  = past_mean(data['vapour_pressure_deficit'], 3)\n",
    "data['vapour_pressure_deficit_roll_mean_24h'] = past_mean(data['vapour_pressure_deficit'], 24)\n",
    "\n",
    "# clouds/wind/sun\n",
    "data['cloud_cover_roll_mean_3h']      = past_mean(data['cloud_cover'], 3)\n",
    "data['cloud_cover_low_roll_mean_3h']  = past_mean(data['cloud_cover_low'], 3)\n",
    "data['cloud_cover_mid_roll_mean_3h']  = past_mean(data['cloud_cover_mid'], 3)\n",
    "data['cloud_cover_high_roll_mean_3h'] = past_mean(data['cloud_cover_high'], 3)\n",
    "data['wind_gusts_10m_roll_mean_3h']   = past_mean(data['wind_gusts_10m'], 3)\n",
    "data['wind_speed_10m_roll_mean_3h']   = past_mean(data['wind_speed_10m'], 3)\n",
    "data['sunshine_duration_roll_mean_3h'] = past_mean(data['sunshine_duration'], 3)\n",
    "data['et0_fao_evapotranspiration_roll_mean_3h'] = past_mean(data['et0_fao_evapotranspiration'], 3)\n",
    "\n",
    "# precipitation (sum)\n",
    "data['precipitation_roll_sum_3h'] = past_sum(data['precipitation'], 3)\n",
    "\n",
    "# safe lag\n",
    "data['load_lag_1h']   = data['load'].shift(1)\n",
    "data['load_lag_2h']   = data['load'].shift(2)\n",
    "data['load_lag_3h']   = data['load'].shift(3)\n",
    "data['load_lag_24h']  = data['load'].shift(24)\n",
    "data['load_lag_25h']  = data['load'].shift(25)\n",
    "data['load_lag_168h'] = data['load'].shift(168)\n",
    "\n",
    "data['pressure_msl_lag_24h']            = data['pressure_msl'].shift(24)\n",
    "data['temperature_2m_lag_24h']          = data['temperature_2m'].shift(24)\n",
    "data['relative_humidity_2m_lag_1h']     = data['relative_humidity_2m'].shift(1)\n",
    "data['relative_humidity_2m_lag_24h']    = data['relative_humidity_2m'].shift(24)\n",
    "data['vapour_pressure_deficit_lag_1h']  = data['vapour_pressure_deficit'].shift(1)\n",
    "data['vapour_pressure_deficit_lag_24h'] = data['vapour_pressure_deficit'].shift(24)\n",
    "data['apparent_temperature_lag_24h']    = data['apparent_temperature'].shift(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e240d555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (10775, 68)\n",
      "Has time_idx: True\n",
      "Has group_id: True\n",
      "Has load: True\n",
      "\n",
      "First few columns: ['load', 'temperature_2m', 'apparent_temperature', 'relative_humidity_2m', 'vapour_pressure_deficit', 'pressure_msl', 'precipitation', 'cloud_cover', 'cloud_cover_low', 'cloud_cover_mid']\n"
     ]
    }
   ],
   "source": [
    "data.dropna(inplace=True)\n",
    "\n",
    "# Drop date column as TimeSeriesDataSet uses time_idx\n",
    "data = data.drop(['date'], axis=1)\n",
    "\n",
    "# Verify we have time_idx and group_id\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Has time_idx: {'time_idx' in data.columns}\")\n",
    "print(f\"Has group_id: {'group_id' in data.columns}\")\n",
    "print(f\"Has load: {'load' in data.columns}\")\n",
    "print(f\"\\nFirst few columns: {list(data.columns[:10])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "68dd80f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known reals: 49 features\n",
      "Unknown reals: 12 features\n",
      "\n",
      "Training samples: 10774\n",
      "Validation samples: 1\n"
     ]
    }
   ],
   "source": [
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "\n",
    "# Identify all feature columns (exclude target, time_idx, group_id, and time features we'll use as static)\n",
    "exclude_cols = {'load', 'time_idx', 'group_id', 'year', 'month', 'day', 'hour', 'day_of_week'}\n",
    "feature_cols = [col for col in data.columns if col not in exclude_cols]\n",
    "\n",
    "# Separate known future features (can be forecasted/known in advance) from unknown (observed only)\n",
    "# Known features: weather forecasts, time-based cyclical features, rolling features from past\n",
    "known_reals = [\n",
    "    # Weather features (assumed to be forecastable)\n",
    "    \"temperature_2m\", \"apparent_temperature\", \"relative_humidity_2m\",\n",
    "    \"vapour_pressure_deficit\", \"pressure_msl\", \"precipitation\",\n",
    "    \"cloud_cover\", \"cloud_cover_low\", \"cloud_cover_mid\", \"cloud_cover_high\",\n",
    "    \"et0_fao_evapotranspiration\", \"sunshine_duration\",\n",
    "    \"wind_speed_10m\", \"wind_gusts_10m\",\n",
    "    # Cyclical time features\n",
    "    \"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\", \"month_sin\", \"month_cos\",\n",
    "    # Wind direction features\n",
    "    \"wind_dir_cos_10m\", \"wind_dir_sin_10m\",\n",
    "    # Rolling features (derived from past, but computed for future too)\n",
    "    \"wind_dir_cos_10m_roll_mean_3h\", \"wind_dir_sin_10m_roll_mean_3h\",\n",
    "    \"pressure_msl_roll_mean_3h\", \"pressure_msl_roll_mean_24h\",\n",
    "    \"temperature_2m_roll_mean_3h\", \"temperature_2m_roll_mean_24h\",\n",
    "    \"apparent_temperature_roll_mean_3h\",\n",
    "    \"relative_humidity_2m_roll_mean_3h\", \"relative_humidity_2m_roll_mean_24h\",\n",
    "    \"vapour_pressure_deficit_roll_mean_3h\", \"vapour_pressure_deficit_roll_mean_24h\",\n",
    "    \"cloud_cover_roll_mean_3h\", \"cloud_cover_low_roll_mean_3h\",\n",
    "    \"cloud_cover_mid_roll_mean_3h\", \"cloud_cover_high_roll_mean_3h\",\n",
    "    \"wind_gusts_10m_roll_mean_3h\", \"wind_speed_10m_roll_mean_3h\",\n",
    "    \"sunshine_duration_roll_mean_3h\", \"et0_fao_evapotranspiration_roll_mean_3h\",\n",
    "    \"precipitation_roll_sum_3h\",\n",
    "    # Lag features (from past observations)\n",
    "    \"pressure_msl_lag_24h\", \"temperature_2m_lag_24h\",\n",
    "    \"relative_humidity_2m_lag_1h\", \"relative_humidity_2m_lag_24h\",\n",
    "    \"vapour_pressure_deficit_lag_1h\", \"vapour_pressure_deficit_lag_24h\",\n",
    "    \"apparent_temperature_lag_24h\",\n",
    "]\n",
    "\n",
    "# Unknown features: historical load values (observed only, not forecastable)\n",
    "unknown_reals = [\n",
    "    \"load\",  # Target variable (past values only)\n",
    "    # Load-based features (derived from historical load)\n",
    "    \"load_lag_1h\", \"load_lag_2h\", \"load_lag_3h\", \"load_lag_24h\", \"load_lag_25h\", \"load_lag_168h\",\n",
    "    \"load_roll_std_3h\", \"load_roll_std_24h\", \"load_roll_mean_24h\",\n",
    "    \"load_roll_std_168h\", \"load_roll_mean_168h\",\n",
    "]\n",
    "\n",
    "# Filter to only include columns that actually exist in the dataframe\n",
    "known_reals = [col for col in known_reals if col in data.columns]\n",
    "unknown_reals = [col for col in unknown_reals if col in data.columns]\n",
    "\n",
    "print(f\"Known reals: {len(known_reals)} features\")\n",
    "print(f\"Unknown reals: {len(unknown_reals)} features\")\n",
    "\n",
    "# Create TimeSeriesDataSet\n",
    "max_encoder_length = 168  # 1 week of historical data\n",
    "max_prediction_length = 24  # Predict 24 hours ahead\n",
    "\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[data.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"load\",\n",
    "    group_ids=[\"group_id\"], \n",
    "    min_encoder_length=max_encoder_length // 2,  # Minimum encoder length\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[],  # No static categorical features\n",
    "    static_reals=[],  # Can add static real-valued features if needed\n",
    "    time_varying_known_reals=known_reals,\n",
    "    time_varying_unknown_reals=unknown_reals,\n",
    "    target_normalizer=None,  # Will use default GroupNormalizer\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "# Create validation dataset\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "\n",
    "print(f\"\\nTraining samples: {len(training)}\")\n",
    "print(f\"Validation samples: {len(validation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a64dabe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 64\n",
      "Train batches: 168\n",
      "Validation batches: 1\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "batch_size = 64  # Adjust based on your GPU memory\n",
    "\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)\n",
    "\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Train batches: {len(train_dataloader)}\")\n",
    "print(f\"Validation batches: {len(val_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9d61f960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\atmor\\OneDrive\\Desktop\\Repositories\\Prenergyze\\prenergyze_venv\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:210: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "c:\\Users\\atmor\\OneDrive\\Desktop\\Repositories\\Prenergyze\\prenergyze_venv\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:210: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 94.1k\n"
     ]
    }
   ],
   "source": [
    "from pytorch_forecasting import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "import torch\n",
    "\n",
    "# Configure TFT model\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,  # Reduced for faster training, increase to 64-128 for better performance\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,  # Size for processing continuous variables\n",
    "    output_size=7,  # 7 quantiles by default: [0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98]\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {tft.size()/1e3:.1f}k\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7b7991",
   "metadata": {},
   "source": [
    "## IMPORTANT: PyTorch Lightning Version Compatibility\n",
    "\n",
    "**If you encounter errors**, the best solution is to downgrade PyTorch Lightning:\n",
    "```python\n",
    "!pip install pytorch-lightning==1.9.0\n",
    "```\n",
    "\n",
    "Then restart your kernel and re-run the cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "da7a07df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: PyTorch Lightning 2.5.6 has compatibility issues with pytorch-forecasting 1.5.0\n",
      "For best results, downgrade to pytorch-lightning==1.9.0:\n",
      "  pip install pytorch-lightning==1.9.0\n",
      "\n",
      "Attempting workaround...\n",
      "Applied compatibility patch (may not work - consider downgrading)\n",
      "Starting training...\n",
      "Workaround failed: maximum recursion depth exceeded\n",
      "Please downgrade to pytorch-lightning==1.9.0 for compatibility\n"
     ]
    },
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRecursionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 115\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Use appropriate parameter names based on version\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m major_version >= \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    121\u001b[39m     trainer.fit(\n\u001b[32m    122\u001b[39m         tft,\n\u001b[32m    123\u001b[39m         train_dataloader=train_dataloader,\n\u001b[32m    124\u001b[39m         val_dataloaders=val_dataloader,\n\u001b[32m    125\u001b[39m     )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mpatched_fit\u001b[39m\u001b[34m(self, model, *args, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m wrapped = TFTWrapper(model)\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moriginal_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     71\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWorkaround failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[114]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mpatched_fit\u001b[39m\u001b[34m(self, model, *args, **kwargs)\u001b[39m\n\u001b[32m     60\u001b[39m         model.\u001b[34m__class__\u001b[39m = original_class\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;66;03m# For other models, use original\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moriginal_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[114]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mpatched_fit\u001b[39m\u001b[34m(self, model, *args, **kwargs)\u001b[39m\n\u001b[32m     60\u001b[39m         model.\u001b[34m__class__\u001b[39m = original_class\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;66;03m# For other models, use original\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moriginal_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping similar frames: patched_fit at line 63 (2974 times)]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[114]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mpatched_fit\u001b[39m\u001b[34m(self, model, *args, **kwargs)\u001b[39m\n\u001b[32m     60\u001b[39m         model.\u001b[34m__class__\u001b[39m = original_class\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;66;03m# For other models, use original\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moriginal_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRecursionError\u001b[39m: maximum recursion depth exceeded"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atmor\\OneDrive\\Desktop\\Repositories\\Prenergyze\\prenergyze_venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py:128\u001b[39m, in \u001b[36m_pseudo_sync_runner\u001b[39m\u001b[34m(coro)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[33;03mA runner that does not really allow async execution, and just advance the coroutine.\u001b[39;00m\n\u001b[32m    122\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    125\u001b[39m \u001b[33;03mCredit to Nathaniel Smith\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc.value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atmor\\OneDrive\\Desktop\\Repositories\\Prenergyze\\prenergyze_venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3413\u001b[39m, in \u001b[36mInteractiveShell.run_cell_async\u001b[39m\u001b[34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001b[39m\n\u001b[32m   3409\u001b[39m exec_count = \u001b[38;5;28mself\u001b[39m.execution_count\n\u001b[32m   3410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.error_in_exec:\n\u001b[32m   3411\u001b[39m     \u001b[38;5;66;03m# Store formatted traceback and error details\u001b[39;00m\n\u001b[32m   3412\u001b[39m     \u001b[38;5;28mself\u001b[39m.history_manager.exceptions[exec_count] = (\n\u001b[32m-> \u001b[39m\u001b[32m3413\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_exception_for_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror_in_exec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3414\u001b[39m     )\n\u001b[32m   3416\u001b[39m \u001b[38;5;66;03m# Each cell is a *single* input, regardless of how many lines it has\u001b[39;00m\n\u001b[32m   3417\u001b[39m \u001b[38;5;28mself\u001b[39m.execution_count += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atmor\\OneDrive\\Desktop\\Repositories\\Prenergyze\\prenergyze_venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3467\u001b[39m, in \u001b[36mInteractiveShell._format_exception_for_storage\u001b[39m\u001b[34m(self, exception, filename, running_compiled_code)\u001b[39m\n\u001b[32m   3464\u001b[39m         stb = evalue._render_traceback_()\n\u001b[32m   3465\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3466\u001b[39m         \u001b[38;5;66;03m# Otherwise, use InteractiveTB to format the traceback.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3467\u001b[39m         stb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mInteractiveTB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3468\u001b[39m \u001b[43m            \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m   3469\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3470\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   3471\u001b[39m     \u001b[38;5;66;03m# In case formatting fails, fallback to Python's built-in formatting.\u001b[39;00m\n\u001b[32m   3472\u001b[39m     stb = traceback.format_exception(etype, evalue, tb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atmor\\OneDrive\\Desktop\\Repositories\\Prenergyze\\prenergyze_venv\\Lib\\site-packages\\IPython\\core\\ultratb.py:1188\u001b[39m, in \u001b[36mAutoFormattedTB.structured_traceback\u001b[39m\u001b[34m(self, etype, evalue, etb, tb_offset, context)\u001b[39m\n\u001b[32m   1186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1187\u001b[39m     \u001b[38;5;28mself\u001b[39m.tb = etb\n\u001b[32m-> \u001b[39m\u001b[32m1188\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFormattedTB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atmor\\OneDrive\\Desktop\\Repositories\\Prenergyze\\prenergyze_venv\\Lib\\site-packages\\IPython\\core\\ultratb.py:1059\u001b[39m, in \u001b[36mFormattedTB.structured_traceback\u001b[39m\u001b[34m(self, etype, evalue, etb, tb_offset, context)\u001b[39m\n\u001b[32m   1056\u001b[39m mode = \u001b[38;5;28mself\u001b[39m.mode\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose_modes:\n\u001b[32m   1058\u001b[39m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1059\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVerboseTB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1060\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[32m   1061\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1062\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mDocs\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1063\u001b[39m     \u001b[38;5;66;03m# return DocTB\u001b[39;00m\n\u001b[32m   1064\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DocTB(\n\u001b[32m   1065\u001b[39m         theme_name=\u001b[38;5;28mself\u001b[39m._theme_name,\n\u001b[32m   1066\u001b[39m         call_pdb=\u001b[38;5;28mself\u001b[39m.call_pdb,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1074\u001b[39m         etype, evalue, etb, tb_offset, \u001b[32m1\u001b[39m\n\u001b[32m   1075\u001b[39m     )  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atmor\\OneDrive\\Desktop\\Repositories\\Prenergyze\\prenergyze_venv\\Lib\\site-packages\\IPython\\core\\ultratb.py:867\u001b[39m, in \u001b[36mVerboseTB.structured_traceback\u001b[39m\u001b[34m(self, etype, evalue, etb, tb_offset, context)\u001b[39m\n\u001b[32m    858\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstructured_traceback\u001b[39m(\n\u001b[32m    859\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    860\u001b[39m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    864\u001b[39m     context: \u001b[38;5;28mint\u001b[39m = \u001b[32m5\u001b[39m,\n\u001b[32m    865\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    866\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m867\u001b[39m     formatted_exceptions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_exception_as_a_whole\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m        \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    871\u001b[39m     termsize = \u001b[38;5;28mmin\u001b[39m(\u001b[32m75\u001b[39m, get_terminal_size()[\u001b[32m0\u001b[39m])\n\u001b[32m    872\u001b[39m     theme = theme_table[\u001b[38;5;28mself\u001b[39m._theme_name]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atmor\\OneDrive\\Desktop\\Repositories\\Prenergyze\\prenergyze_venv\\Lib\\site-packages\\IPython\\core\\ultratb.py:752\u001b[39m, in \u001b[36mVerboseTB.format_exception_as_a_whole\u001b[39m\u001b[34m(self, etype, evalue, etb, context, tb_offset)\u001b[39m\n\u001b[32m    750\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[32m    751\u001b[39m head = \u001b[38;5;28mself\u001b[39m.prepare_header(\u001b[38;5;28mstr\u001b[39m(etype), \u001b[38;5;28mself\u001b[39m.long_header)\n\u001b[32m--> \u001b[39m\u001b[32m752\u001b[39m records = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m    754\u001b[39m frames = []\n\u001b[32m    755\u001b[39m skipped = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\atmor\\OneDrive\\Desktop\\Repositories\\Prenergyze\\prenergyze_venv\\Lib\\site-packages\\IPython\\core\\ultratb.py:835\u001b[39m, in \u001b[36mVerboseTB.get_records\u001b[39m\u001b[34m(self, etb, context, tb_offset)\u001b[39m\n\u001b[32m    832\u001b[39m     max_len = get_line_number_of_frame(cf.tb_frame)\n\u001b[32m    834\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m     max_len = \u001b[32m0\u001b[39m\n\u001b[32m    836\u001b[39m max_len = \u001b[38;5;28mmax\u001b[39m(max_len, max_len)\n\u001b[32m    837\u001b[39m tbs.append(cf)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "# Workaround for pytorch-lightning 2.x compatibility with pytorch-forecasting 1.5.0\n",
    "# The issue: pytorch-lightning 2.x has stricter type checking that rejects TFT\n",
    "# Solution: Wrap TFT model to pass isinstance checks\n",
    "pl_version = pl.__version__\n",
    "major_version = int(pl_version.split('.')[0])\n",
    "\n",
    "if major_version >= 2:\n",
    "    # PyTorch Lightning 2.x compatibility workaround\n",
    "    # The simplest solution: downgrade to pytorch-lightning 1.9.0\n",
    "    # But if you must use 2.x, we'll use a direct wrapper approach\n",
    "    print(f\"WARNING: PyTorch Lightning {pl_version} has compatibility issues with pytorch-forecasting 1.5.0\")\n",
    "    print(\"For best results, downgrade to pytorch-lightning==1.9.0:\")\n",
    "    print(\"  pip install pytorch-lightning==1.9.0\")\n",
    "    print(\"\")\n",
    "    print(\"Attempting workaround...\")\n",
    "    \n",
    "    # Import TFT class\n",
    "    from pytorch_forecasting import TemporalFusionTransformer\n",
    "    TFT_TYPE = TemporalFusionTransformer\n",
    "    \n",
    "    # Store original fit\n",
    "    original_fit = Trainer.fit\n",
    "    \n",
    "    # Simple wrapper class that properly delegates everything\n",
    "    class TFTWrapper(pl.LightningModule):\n",
    "        \"\"\"Wrapper that makes TFT pass LightningModule checks\"\"\"\n",
    "        def __init__(self, tft_model):\n",
    "            # Don't call super().__init__() to avoid conflicts\n",
    "            object.__setattr__(self, '_tft', tft_model)\n",
    "            # Copy all attributes\n",
    "            for key, value in tft_model.__dict__.items():\n",
    "                object.__setattr__(self, key, value)\n",
    "        \n",
    "        def __getattribute__(self, name):\n",
    "            if name == '_tft':\n",
    "                return object.__getattribute__(self, '_tft')\n",
    "            if name in ('__dict__', '__class__'):\n",
    "                return object.__getattribute__(self, name)\n",
    "            # Delegate to wrapped TFT\n",
    "            return getattr(self._tft, name)\n",
    "        \n",
    "        def __setattr__(self, name, value):\n",
    "            if name == '_tft':\n",
    "                object.__setattr__(self, name, value)\n",
    "                return\n",
    "            # Set on wrapped TFT\n",
    "            setattr(self._tft, name, value)\n",
    "            # Also set on self for attributes that might be checked\n",
    "            try:\n",
    "                object.__setattr__(self, name, value)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    def patched_fit(self, model, *args, **kwargs):\n",
    "        # Check if it's TFT\n",
    "        model_type = type(model)\n",
    "        is_tft = (model_type == TFT_TYPE or \n",
    "                  model_type.__name__ == 'TemporalFusionTransformer')\n",
    "        \n",
    "        if is_tft:\n",
    "            # Wrap the TFT model\n",
    "            wrapped = TFTWrapper(model)\n",
    "            try:\n",
    "                return original_fit(self, wrapped, *args, **kwargs)\n",
    "            except Exception as e:\n",
    "                print(f\"Workaround failed: {e}\")\n",
    "                print(\"Please downgrade to pytorch-lightning==1.9.0 for compatibility\")\n",
    "                raise\n",
    "        else:\n",
    "            return original_fit(self, model, *args, **kwargs)\n",
    "    \n",
    "    Trainer.fit = patched_fit\n",
    "    print(\"Applied compatibility patch (may not work - consider downgrading)\")\n",
    "else:\n",
    "    print(f\"Using PyTorch Lightning {pl_version} (1.x - no patch needed)\")\n",
    "\n",
    "# Configure trainer with callbacks\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "if major_version >= 2:\n",
    "    # PyTorch Lightning 2.x\n",
    "    trainer = Trainer(\n",
    "        max_epochs=30,\n",
    "        accelerator=\"auto\",  # Automatically detects GPU if available\n",
    "        enable_model_summary=True,\n",
    "        gradient_clip_val=0.1,\n",
    "        callbacks=[early_stop_callback, checkpoint_callback],\n",
    "        limit_train_batches=50,  # Limit for faster iteration during development\n",
    "    )\n",
    "else:\n",
    "    # PyTorch Lightning 1.x\n",
    "    trainer = Trainer(\n",
    "        max_epochs=30,\n",
    "        gpus=1 if torch.cuda.is_available() else 0,\n",
    "        enable_model_summary=True,\n",
    "        gradient_clip_val=0.1,\n",
    "        callbacks=[early_stop_callback, checkpoint_callback],\n",
    "        limit_train_batches=50,\n",
    "    )\n",
    "\n",
    "print(\"Starting training...\")\n",
    "# Use appropriate parameter names based on version\n",
    "if major_version >= 2:\n",
    "    trainer.fit(\n",
    "        tft,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=val_dataloader,\n",
    "    )\n",
    "else:\n",
    "    trainer.fit(\n",
    "        tft,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloaders=val_dataloader,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b89a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model from checkpoint\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "print(f\"Best model path: {best_model_path}\")\n",
    "\n",
    "# Generate predictions (returns quantiles, use median/0.5 quantile for point predictions)\n",
    "raw_predictions = tft.predict(val_dataloader)\n",
    "# Extract median (0.5 quantile) which is at index 3 for 7 quantiles: [0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98]\n",
    "predictions = raw_predictions[..., 3] if raw_predictions.dim() > 2 else raw_predictions\n",
    "\n",
    "# Get actual values for comparison\n",
    "actuals_list = []\n",
    "for x, (y, weight) in iter(val_dataloader):\n",
    "    actuals_list.append(y)\n",
    "actuals = torch.cat(actuals_list, dim=0)\n",
    "\n",
    "# Calculate metrics\n",
    "from pytorch_forecasting.metrics import MAE, RMSE, MAPE\n",
    "\n",
    "mae_metric = MAE()\n",
    "rmse_metric = RMSE()\n",
    "mape_metric = MAPE()\n",
    "\n",
    "mae = mae_metric(predictions, actuals)\n",
    "rmse = rmse_metric(predictions, actuals)\n",
    "mape = mape_metric(predictions, actuals)\n",
    "\n",
    "print(f\"\\nValidation Metrics:\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAPE: {mape:.2%}\")\n",
    "\n",
    "# Visualize predictions vs actuals\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx in range(min(4, len(predictions))):\n",
    "    ax = axes[idx]\n",
    "    actual_vals = actuals[idx].cpu().numpy()\n",
    "    pred_vals = predictions[idx].cpu().numpy()\n",
    "    ax.plot(actual_vals, label='Actual', marker='o', alpha=0.7)\n",
    "    ax.plot(pred_vals, label='Predicted', marker='x', alpha=0.7)\n",
    "    ax.set_title(f'Sample {idx+1} - MAE: {np.abs(actual_vals - pred_vals).mean():.2f}')\n",
    "    ax.set_xlabel('Hour ahead')\n",
    "    ax.set_ylabel('Load')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot overall prediction accuracy\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "all_actuals = actuals.cpu().numpy().flatten()\n",
    "all_predictions = predictions.cpu().numpy().flatten()\n",
    "ax.scatter(all_actuals, all_predictions, alpha=0.5, s=10)\n",
    "ax.plot([all_actuals.min(), all_actuals.max()], \n",
    "        [all_actuals.min(), all_actuals.max()], 'r--', lw=2, label='Perfect prediction')\n",
    "ax.set_xlabel('Actual Load')\n",
    "ax.set_ylabel('Predicted Load')\n",
    "ax.set_title(f'Prediction Accuracy (Overall RMSE: {rmse:.2f})')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prenergyze_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
